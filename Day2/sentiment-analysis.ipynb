{"cells":[{"cell_type":"markdown","metadata":{"id":"8mahmVSp3R7B"},"source":["# Table of Contents\n","0. [Introduction](#introduction)\n","1. [pandas](#Pandas)\n","2. [Matplotlib](#Matplotlib)\n","3. [sklearn](#sklearn)\n","4. [Modeling](#Modeling)\n","5. [Deployment](#Deployment)\n","6. [References](#References)"]},{"cell_type":"markdown","metadata":{"id":"PY78yIJa3R7L"},"source":["# Introduction\n","\n","## Task 2 Steps\n","5. ML training and validation - a code base to train topic modelling and sentiment analysis models.\n","6. Deployment - a code base to make and deploy dashboard expose trained models. \n","7. Model performance analyser - to integrate MLWatcher or something similar to monitor model performance at prediction time. \n","8. A data drift or model underperformance trigger mechanism that identifies and shows an alert when model performance is below threshold or incoming data is drifted from the data used to train the model."]},{"cell_type":"markdown","metadata":{"id":"4JlRzW-q3R7N"},"source":["## Data Gathering"]},{"cell_type":"markdown","metadata":{"id":"iHLDlq9x3R7O"},"source":["### Downloading & extracting data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgP2dBPR3R7P","outputId":"37b9e69a-e56d-418e-fad8-0c645e98d7f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=fc48aa6ae4cd108e0eb9c9b8266bc849e441a90c437954cafb72fbfa0acdb34e\n","  Stored in directory: /Users/gciniwe/Library/Caches/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"]}],"source":["#!pip install wget"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcshyQkg3R7S","outputId":"dd6da647-b010-4bd4-c635-8e6bd7e8c98c"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-04-26 13:04:24--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n","Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 84125825 (80M) [application/x-gzip]\n","Saving to: ‘aclImdb_v1.tar.gz’\n","\n","aclImdb_v1.tar.gz   100%[===================>]  80.23M  8.90MB/s    in 13s     \n","\n","2022-04-26 13:04:37 (6.34 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n","\n"]}],"source":["!wget \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\" "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ov5MME_63R7U","outputId":"ca23aed9-d211-4f1b-c96c-d5cc850e57bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 80.2M  100 80.2M    0     0   666k      0  0:02:03  0:02:03 --:--:--  513k    0  0:01:24  0:00:21  0:01:03 1313k  901k      0  0:01:31  0:00:34  0:00:57 1365kk      0  0:01:29  0:00:36  0:00:53 1262k927k      0  0:01:28  0:00:53  0:00:35  771k01:23  0:01:00  0:00:23 1380k1k      0  0:01:28  0:01:17  0:00:11  2613  0  0:01:47  0:01:34  0:00:13 922260     0   719k      0  0:01:54  0:01:42  0:00:12  227k\n"]}],"source":["#!curl \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\" -o aclImdb_v1.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yGuwGZqA3R7V"},"outputs":[],"source":["#!tar -xzf \"aclImdb_v1.tar.gz\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1TFoUp073R7X","outputId":"b18a740a-47da-4b53-fc33-dccfade78921"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n","p7zip Version 16.02 (locale=en_ZA.UTF-8,Utf16=on,HugeFiles=on,64 bits,8 CPUs Intel(R) Core(TM) i7-4770 CPU @ 3.40GHz (306C3),ASM,AES-NI)\n","\n","Scanning the drive for archives:\n","  0M Sca        1 file, 84125825 bytes (81 MiB)\n","\n","Extracting archive: aclImdb_v1.tar.gz\n","--\n","Path = aclImdb_v1.tar.gz\n","Type = gzip\n","Headers Size = 10\n","\n","  0% - aclImdb_v1.ta                     12% - aclImdb_v1.ta                     26% - aclImdb_v1.ta                     38% - aclImdb_v1.ta                     51% - aclImdb_v1.ta                     63% - aclImdb_v1.ta                     75% - aclImdb_v1.ta                     87% - aclImdb_v1.ta                    Everything is Ok\n","\n","Size:       298168320\n","Compressed: 84125825\n"]}],"source":["!7z x aclImdb_v1.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V3ZM5_qT3R7Y","outputId":"47f35a91-f7d9-498e-8d42-98f058fdd253"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 379004\r\n","drwxrwxr-x 8 musa musa      4096 Apr 26 13:05  .\r\n","drwxrwxr-x 9 musa musa      4096 Apr 22 11:42  ..\r\n","drwxr-xr-x 4 musa musa      4096 Apr 26 05:30  aclImdb\r\n","-rw-rw-r-- 1 musa musa 298168320 Jun 26  2011  aclImdb_v1.tar\r\n","-rw-rw-r-- 1 musa musa  84125825 Jun 26  2011  aclImdb_v1.tar.gz\r\n","-rw-r--r-- 1 musa musa     11242 Apr 26 03:49 'Challenge_ Day2.ipynb'\r\n","drwxr-xr-x 2 musa musa      4096 Apr 26 06:07  classifiers\r\n","-rw-r--r-- 1 musa musa   3960754 Apr 23 03:23  cleaned_fintech_data.csv\r\n","-rw-r--r-- 1 musa musa     55131 Apr 23 03:23  CRISP-DM.pptx\r\n","drwxr-xr-x 2 musa musa      4096 Apr 26 05:31  csv\r\n","drwxr-xr-x 2 musa musa      4096 Apr 26 05:46  data_preprocessors\r\n","-rw-rw-r-- 1 musa musa    600641 Apr 26 01:08  DS-Workflow.pptx\r\n","-rw-r--r-- 1 musa musa    810716 Apr 26 03:52  EDA-Twitter.ipynb\r\n","drwxrwxr-x 2 musa musa      4096 Apr 26 11:06  .ipynb_checkpoints\r\n","-rw-r--r-- 1 musa musa     42310 Apr 26 13:05  sentiment-analysis.ipynb\r\n","-rw-rw-r-- 1 musa musa    277585 Apr 26 10:30  sentiment-analysis.pdf\r\n","drwxr-xr-x 2 musa musa      4096 Apr 26 05:47  vectorized_data\r\n"]}],"source":["!ls -la"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpmw2cZz3R7a","outputId":"67b7491a-571c-4cb9-e929-3ab5c10ad53b"},"outputs":[{"name":"stdout","output_type":"stream","text":["imdbEr.txt  imdb.vocab\tREADME\ttest  train\r\n"]}],"source":["!ls aclImdb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RA7GsJjb3R7b","outputId":"f23b131f-3c8f-4acb-fafa-350e4306649d"},"outputs":[{"name":"stdout","output_type":"stream","text":["labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\r\n","neg\t\t unsup\turls_neg.txt   urls_unsup.txt\r\n"]}],"source":["!ls aclImdb/train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X85ijsPm3R7b","outputId":"35a51270-628a-403a-85f7-25467365302c"},"outputs":[{"name":"stdout","output_type":"stream","text":["labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\r\n"]}],"source":["!ls aclImdb/test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O6ZY4_Hj3R7c","outputId":"416e4e47-48e6-46ff-bf07-fd2db78f26c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.0490972013402\r\n","0.201363575849\r\n","0.0333946807184\r\n","0.099837669572\r\n","-0.0790210365788\r\n","0.188660139871\r\n","0.00712569582356\r\n","0.109215821589\r\n","-0.154986397986\r\n","-0.222690363917\r\n"]}],"source":["!head -10 aclImdb/imdbEr.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwK4z5RM3R7d","outputId":"11441ae2-d4ca-4fae-8a0f-3231e16f4fb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["the\r\n","and\r\n","a\r\n","of\r\n","to\r\n","is\r\n","it\r\n","in\r\n","i\r\n","this\r\n"]}],"source":["!head -10 aclImdb/imdb.vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_W0qLi8W3R7e","outputId":"6664ea16-de47-42ac-87de-4539747ebf5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Large Movie Review Dataset v1.0\r\n","\r\n","Overview\r\n","\r\n","This dataset contains movie reviews along with their associated binary\r\n","sentiment polarity labels. It is intended to serve as a benchmark for\r\n","sentiment classification. This document outlines how the dataset was\r\n","gathered, and how to use the files provided. \r\n","\r\n","Dataset \r\n","\r\n","The core dataset contains 50,000 reviews split evenly into 25k train\r\n","and 25k test sets. The overall distribution of labels is balanced (25k\r\n","pos and 25k neg). We also include an additional 50,000 unlabeled\r\n","documents for unsupervised learning. \r\n","\r\n","In the entire collection, no more than 30 reviews are allowed for any\r\n","given movie because reviews for the same movie tend to have correlated\r\n","ratings. Further, the train and test sets contain a disjoint set of\r\n","movies, so no significant performance is obtained by memorizing\r\n","movie-unique terms and their associated with observed labels.  In the\r\n","labeled train/test sets, a negative review has a score <= 4 out of 10,\r\n","and a positive review has a score >= 7 out of 10. Thus reviews with\r\n","more neutral ratings are not included in the train/test sets. In the\r\n","unsupervised set, reviews of any rating are included and there are an\r\n","even number of reviews > 5 and <= 5.\r\n","\r\n","Files\r\n","\r\n","There are two top-level directories [train/, test/] corresponding to\r\n","the training and test sets. Each contains [pos/, neg/] directories for\r\n","the reviews with binary labels positive and negative. Within these\r\n","directories, reviews are stored in text files named following the\r\n","convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\r\n","the star rating for that review on a 1-10 scale. For example, the file\r\n","[test/pos/200_8.txt] is the text for a positive-labeled test set\r\n","example with unique id 200 and star rating 8/10 from IMDb. The\r\n","[train/unsup/] directory has 0 for all ratings because the ratings are\r\n","omitted for this portion of the dataset.\r\n","\r\n","We also include the IMDb URLs for each review in a separate\r\n","[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\r\n","have its URL on line 200 of this file. Due the ever-changing IMDb, we\r\n","are unable to link directly to the review, but only to the movie's\r\n","review page.\r\n","\r\n","In addition to the review text files, we include already-tokenized bag\r\n","of words (BoW) features that were used in our experiments. These \r\n","are stored in .feat files in the train/test directories. Each .feat\r\n","file is in LIBSVM format, an ascii sparse-vector format for labeled\r\n","data.  The feature indices in these files start from 0, and the text\r\n","tokens corresponding to a feature index is found in [imdb.vocab]. So a\r\n","line with 0:7 in a .feat file means the first word in [imdb.vocab]\r\n","(the) appears 7 times in that review.\r\n","\r\n","LIBSVM page for details on .feat file format:\r\n","http://www.csie.ntu.edu.tw/~cjlin/libsvm/\r\n","\r\n","We also include [imdbEr.txt] which contains the expected rating for\r\n","each token in [imdb.vocab] as computed by (Potts, 2011). The expected\r\n","rating is a good way to get a sense for the average polarity of a word\r\n","in the dataset.\r\n","\r\n","Citing the dataset\r\n","\r\n","When using this dataset please cite our ACL 2011 paper which\r\n","introduces it. This paper also contains classification results which\r\n","you may want to compare against.\r\n","\r\n","\r\n","@InProceedings{maas-EtAl:2011:ACL-HLT2011,\r\n","  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\r\n","  title     = {Learning Word Vectors for Sentiment Analysis},\r\n","  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\r\n","  month     = {June},\r\n","  year      = {2011},\r\n","  address   = {Portland, Oregon, USA},\r\n","  publisher = {Association for Computational Linguistics},\r\n","  pages     = {142--150},\r\n","  url       = {http://www.aclweb.org/anthology/P11-1015}\r\n","}\r\n","\r\n","References\r\n","\r\n","Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\r\n","David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\r\n","636-659.\r\n","\r\n","Contact\r\n","\r\n","For questions/comments/corrections please contact Andrew Maas\r\n","amaas@cs.stanford.edu\r\n"]}],"source":["!cat aclImdb/README"]},{"cell_type":"markdown","metadata":{"id":"FEK85vTh3R7e"},"source":["## Environment Setup\n","\n","* Download Week 0 Tuesday folder onto your local filesystem (or wherever you're working)\n","* Create folders you will later need"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9X_yrB-n3R7f"},"outputs":[],"source":["!mkdir 'csv' \n","!mkdir 'data_preprocessors' \n","!mkdir 'vectorized_data' \n","!mkdir 'classifiers'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJTMKKx-3R7g","outputId":"bc8f85b9-ecc0-43ac-8743-269ac6a6e3ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘csv’: File exists\n","mkdir: cannot create directory ‘data_preprocessors’: File exists\n","mkdir: cannot create directory ‘vectorized_data’: File exists\n","mkdir: cannot create directory ‘classifiers’: File exists\n"]}],"source":["!mkdir csv \n","!mkdir data_preprocessors \n","!mkdir vectorized_data \n","!mkdir classifiers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KcnKXgwc3R7h","outputId":"bb22b523-cc6d-4dcc-8e01-644d00957586"},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘csv’: File exists\r\n","mkdir: cannot create directory ‘data_preprocessors’: File exists\r\n","mkdir: cannot create directory ‘vectorized_data’: File exists\r\n","mkdir: cannot create directory ‘classifiers’: File exists\r\n"]}],"source":["!mkdir -p csv data_preprocessors vectorized_data classifiers"]},{"cell_type":"markdown","metadata":{"id":"xBH0G9nD3R7h"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nP4-2vuN3R7i"},"outputs":[],"source":["# pandas library and other Python modules\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import re\n","from os import listdir\n","from os.path import isfile, join\n","from random import shuffle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0NYaxMO3R7i"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.decomposition import NMF, LatentDirichletAllocation\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import RandomizedSearchCV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CAyRmLiN3R7j"},"outputs":[],"source":["import numpy as np # linear algebra\n","from joblib import dump, load # used for saving and loading sklearn objects\n","from scipy.sparse import save_npz, load_npz # used for saving and loading sparse matrices\n","from scipy.stats import uniform\n","from scipy.sparse import csr_matrix"]},{"cell_type":"markdown","metadata":{"id":"-hBquyqk3R7k"},"source":["* <b>joblib</b>: In the specific case of scikit-learn, it may be better to use joblib’s replacement of pickle (dump & load), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string:"]},{"cell_type":"markdown","metadata":{"id":"Avm3m4383R7k"},"source":["# pandas"]},{"cell_type":"markdown","metadata":{"id":"0gt_pz-s3R7k"},"source":["Pandas is a python package for data query and analysis. It is well known for its versatility in reading numerous types of data file. It also allows for simple data munging and visualization.\n","\n","https://pandas.pydata.org/\n","\n","https://pandas.pydata.org/docs/getting_started/index.html\n","\n","https://pandas.pydata.org/docs/getting_started/intro_tutorials/\n","\n","https://pandas.pydata.org/docs/reference/index.html\n","\n","<b> Pandas is heavily dependent on numpy. It borrows its philosophy from R dataframes. </b>"]},{"cell_type":"markdown","metadata":{"id":"sFORe_Pa3R7l"},"source":["### Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFbDjdfw3R7m"},"outputs":[],"source":["def create_data_frame(folder: str) -> pd.DataFrame:\n","    '''\n","    folder - the root folder of train or test dataset\n","    Returns: a DataFrame with the combined data from the input folder\n","    '''\n","    pos_folder = f'{folder}/pos' # positive reviews\n","    neg_folder = f'{folder}/neg' # negative reviews\n","    \n","    def get_files(fld: str) -> list:\n","        '''\n","        fld - positive or negative reviews folder\n","        Returns: a list with all files in input folder\n","        '''\n","        return [join(fld, f) for f in listdir(fld) if isfile(join(fld, f))]\n","    \n","    def append_files_data(data_list: list, files: list, label: int) -> None:\n","        '''\n","        Appends to 'data_list' tuples of form (file content, label)\n","        for each file in 'files' input list\n","        '''\n","        for file_path in files:\n","            with open(file_path, 'r') as f:\n","                text = f.read()\n","                data_list.append((text, label))\n","    \n","    pos_files = get_files(pos_folder)\n","    neg_files = get_files(neg_folder)\n","    \n","    data_list = []\n","    append_files_data(data_list, pos_files, 1)\n","    append_files_data(data_list, neg_files, 0)\n","    shuffle(data_list)\n","    \n","    text, label = tuple(zip(*data_list))\n","    # replacing line breaks with spaces\n","    text = list(map(lambda txt: re.sub('(<br\\s*/?>)+', ' ', txt), text))\n","    \n","    return pd.DataFrame({'text': text, 'label': label})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fd5ESsSG3R7p","outputId":"2742b155-cb0c-4748-e615-f9cc9e31d5ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 330 ms, sys: 54 ms, total: 384 ms\n","Wall time: 415 ms\n"]}],"source":["%%time\n","# imdb_train = create_data_frame('aclImdb/train')\n","# imdb_test = create_data_frame('aclImdb/test')\n","\n","# imdb_train.to_csv('csv/imdb_train.csv', index=False)\n","# imdb_test.to_csv('csv/imdb_test.csv', index=False)\n","\n","imdb_train = pd.read_csv('csv/imdb_train.csv')\n","imdb_test = pd.read_csv('csv/imdb_test.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tk1dpIse3R7q","outputId":"e975969b-17ff-44f9-9411-37753de0be8d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The only reason I wanted to see this was becau...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>One thing i can say about this movie is well l...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I honestly believe that ANYONE considering fil...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>This movie had mediocrity, laziness, and thoug...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I love and admire the Farrelly brothers! How c...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>The story of peace-loving farmers and townspeo...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>what is wrong with you people, if you weren't ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>I was going to give it an 8, but since you peo...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>i just got puzzled why damn FOX canceled the s...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>This movie was strange... I watched it while i...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  label\n","0  The only reason I wanted to see this was becau...      1\n","1  One thing i can say about this movie is well l...      1\n","2  I honestly believe that ANYONE considering fil...      0\n","3  This movie had mediocrity, laziness, and thoug...      0\n","4  I love and admire the Farrelly brothers! How c...      1\n","5  The story of peace-loving farmers and townspeo...      1\n","6  what is wrong with you people, if you weren't ...      1\n","7  I was going to give it an 8, but since you peo...      1\n","8  i just got puzzled why damn FOX canceled the s...      1\n","9  This movie was strange... I watched it while i...      0"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["imdb_train.head(n=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zlAvtgLw3R7q","outputId":"6b136a76-a371-4608-8105-2e785bfa2f47"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Curse of Monkey Island. Released excactly ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>This was like watching the trailer of a up and...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The Capture Of Bigfoot is one of the silliest ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>This is one of the great modern kung fu films....</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>**** MILD SPOILERS _ BUT YOU PROBABLY KNOW THE...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Going into this movie, I was a bit cautious. I...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>A tedious effort from not-yet great director B...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>I fail to understand why anyone would allow a ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>I think that Vanessa Marcil is the best actor ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Ah yet another Seagal movie.In no less than a ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  label\n","0  The Curse of Monkey Island. Released excactly ...      1\n","1  This was like watching the trailer of a up and...      0\n","2  The Capture Of Bigfoot is one of the silliest ...      0\n","3  This is one of the great modern kung fu films....      1\n","4  **** MILD SPOILERS _ BUT YOU PROBABLY KNOW THE...      0\n","5  Going into this movie, I was a bit cautious. I...      1\n","6  A tedious effort from not-yet great director B...      0\n","7  I fail to understand why anyone would allow a ...      0\n","8  I think that Vanessa Marcil is the best actor ...      1\n","9  Ah yet another Seagal movie.In no less than a ...      0"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["imdb_test.head(n=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MUf_oIDh3R7r","outputId":"0f3b874f-187e-4b87-f005-7f30355ebf7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["The only reason I wanted to see this was because of Orlando Bloom. Simply put, the movie was spectacularly average. It's not bad, but it's really not very good. The editing is good; the film is well-paced. The direction is competent and assured. The story is plodding. The film is averagely acted by Ledger, Bloom, and the normally great Watts and Rush. The accents are impenetrable if you're from the US so just sit back and enjoy the scenery (or as I like to call it, Orlando Bloom). By the end of the film, I was neither bored nor moved. Some people have asked what happened to Ned Kelly at the end of the movie. I have to say, I so did not care by that point. Really, the only reason I can recommend this is that Orlando Bloom kind of, sort of shows some hints of range (although the oft-present \"I'm pretty and confused\" look is prominent), so fangirls may find it worth the matinee price. Other than that, just don't see it. It's neither good enough nor bad enough to be entertaining.\n"]}],"source":["print(imdb_train.loc[0, \"text\"])"]},{"cell_type":"markdown","metadata":{"id":"MeJV9-2Y3R7r"},"source":["### loc vs iloc\n","s = pd.Series(list(\"abcdef\"), index=[49, 48, 47, 0, 1, 2])\n","\n","49    a\n","48    b\n","47    c\n","0     d\n","1     e\n","2     f\n","\n","s.loc[0]    # value at index label 0\n","\n","'d'\n","\n","s.iloc[0]   # value at index location 0\n","\n","'a'\n","\n","s.loc[0:1]  # rows at index labels between 0 and 1 (inclusive)\n","\n","0    d\n","1    e\n","\n","s.iloc[0:1] # rows at index location between 0 and 1 (exclusive)\n","\n","49    a\n","\n","Ref: https://stackoverflow.com/questions/31593201/how-are-iloc-and-loc-different#:~:text=The%20main%20distinction%20between%20the,or%20columns)%20at%20integer%20locations."]},{"cell_type":"markdown","metadata":{"id":"UU91xjRM3R7r"},"source":["# Matplotlib"]},{"cell_type":"markdown","metadata":{"id":"B09tJDm33R7s"},"source":["This is exclusively a visualization library in python. It is employed for visual involving 2D, 3D and animation. It also serves as dependency for other numerous libraries.\n","\n","https://matplotlib.org/1.3.1/index.html\n","\n","https://github.com/rougier/matplotlib-tutorial\n","\n","https://matplotlib.org/3.2.2/gallery/index.html\n","\n","Pyplot module is the most used.\n","\n","<b> Other visualisation libraries built on top of Matplotlib: </b>\n","* seaborn\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nh7T6hmz3R7s"},"source":["# sklearn"]},{"cell_type":"markdown","metadata":{"id":"_AvMgN0K3R7s"},"source":["Aka Scikit-Learn. An almost-complete library for data analysis and modelling is the <b>sklearn</b> library. It contains various statistical models and few neural network model one might require in any analytic problem. While thre are many modelling libraries out there, <b>sklearn</b> on it's own houses numerous modelling techniques as modules and functions all which come together to make up the library.\n","\n","https://scikit-learn.org/stable/\n","\n","https://scikit-learn.org/stable/getting_started.html\n","\n","### Other libraries for NLP\n","* HuggingFace\n","* Spacy"]},{"cell_type":"markdown","metadata":{"id":"N6XYNETO3R7t"},"source":["# Modeling"]},{"cell_type":"markdown","metadata":{"id":"ruSliItn3R7t"},"source":["Unlike data wrangling, this is widely the interesting part in general data analysis. The step here are as follows:\n","\n","$\\bullet$ determine the set of algorithms to try on the data (classification, regression, neural-net etc).\n","\n","$\\bullet$ model design - data splitting.\n","\n","$\\bullet$ model building\n","\n","$\\bullet$ evaluation (metrics)\n","\n","$\\bullet$ model review"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LqECynyI3R7t"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"a9jQx2vw3R7u"},"source":["<h2><a href=\"https://github.com/lazuxd/simple-imdb-sentiment-analysis/blob/master/sentiment-analysis.ipynb\"> Notebook reference </a></h2>"]},{"cell_type":"markdown","metadata":{"id":"IFVUNOGQ3R7u"},"source":["# Building a Sentiment Classifier using Scikit-Learn"]},{"cell_type":"markdown","metadata":{"id":"gUNa5k7j3R7u"},"source":["<center><img src=\"https://raw.githubusercontent.com/lazuxd/simple-imdb-sentiment-analysis/master/smiley.jpg\"/></center>\n","<center><i>Image by AbsolutVision @ <a href=\"https://pixabay.com/ro/photos/smiley-emoticon-furie-sup%C4%83rat-2979107/\">pixabay.com</a></i></center>\n","\n","> &nbsp;&nbsp;&nbsp;&nbsp;**Sentiment analysis**, an important area in Natural Language Processing, is the process of automatically detecting affective states of text. Sentiment analysis is widely applied to voice-of-customer materials such as product reviews in online shopping websites like Amazon, movie reviews or social media. It can be just a basic task of classifying the polarity of a text as being positive/negative or it can go beyond polarity, looking at emotional states such as \"happy\", \"angry\", etc.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;Here we will build a classifier that is able to distinguish movie reviews as being either positive or negative. For that, we will use [Large Movie Review Dataset v1.0](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz)<sup>(2)</sup> of IMDB movie reviews.\n","This dataset contains 50,000 movie reviews divided evenly into 25k train and 25k test. The labels are balanced between the two classes (positive and negative). <b>Reviews with a score <= 4 out of 10 are labeled negative and those with score >= 7 out of 10 are labeled positive. Neutral reviews are not included in the labeled data.</b> This dataset also contains unlabeled reviews for unsupervised learning; we will not use them here. <b>There are no more than 30 reviews for a particular movie because the ratings of the same movie tend to be correlated. All reviews for a given movie are either in train or test set but not in both, in order to avoid test accuracy gain by memorizing movie-specific terms.</b>\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4kCpBGc3R7u","outputId":"20c61cbb-902b-4460-b084-03bd4fd3778c"},"outputs":[{"name":"stdout","output_type":"stream","text":["(25000, 2)\n","(25000, 2)\n"]}],"source":["print(imdb_train.shape)\n","print(imdb_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"569EBsdy3R7v"},"source":["## Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"IXiN45T23R7v"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;After the dataset has been downloaded and extracted from archive we have to transform it into a more suitable form for feeding it into a machine learning model for training. We will start by combining all review data into 2 pandas Data Frames representing the train and test datasets, and then saving them as csv files: *imdb_train.csv* and *imdb_test.csv*.  \n","\n","&nbsp;&nbsp;&nbsp;&nbsp;The Data Frames will have the following form:  \n","\n","|text       |label      |\n","|:---------:|:---------:|\n","|review1    |0          |\n","|review2    |1          |\n","|review3    |1          |\n","|.......    |...        |\n","|reviewN    |0          |  \n","\n","&nbsp;&nbsp;&nbsp;&nbsp;where:  \n","- review1, review2, ... = the actual text of movie review  \n","- 0 = negative review  \n","- 1 = positive review"]},{"cell_type":"markdown","metadata":{"id":"ZTuDINNm3R7v"},"source":["<b>But machine learnng algorithms work only with numerical values.</b> We can't just input the text itself into a machine learning model and have it learn from that. We have to, somehow, <b>represent the text by numbers or vectors of numbers</b>. One way of doing this is by using the **Bag-of-words** model<sup>(3)</sup>, in which a piece of text (often called a **document**) is represented by a <b>vector of the counts of words from a vocabulary in that document. This model doesn't take into account grammar rules or word ordering; all it considers is the frequency of words</b>. If we use the counts of each word independently we name this representation a **unigram**. In general, in a **n-gram** we take into account the counts of <b>each combination of n words from the vocabulary that appears in a given document</b>.  \n","\n","&nbsp;&nbsp;&nbsp;&nbsp;For example, consider these two documents:  \n","<br>  \n","<div style=\"font-family: monospace;\"><center><b>d1: \"I am learning\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</b></center></div>  \n","<div style=\"font-family: monospace;\"><center><b>d2: \"Machine learning is cool\"</b></center></div>  \n","<br>\n","The vocabulary of all words encountered in these two sentences is: \n","\n","<br/>  \n","<div style=\"font-family: monospace;\"><center><b>v: [ I, am, learning, machine, is, cool ]</b></center></div>   \n","<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;The unigram representations of d1 and d2:  \n","<br>  \n","\n","|unigram(d1)|I       |am      |learning|machine |is      |cool    |\n","|:---------:|:------:|:------:|:------:|:------:|:------:|:------:|\n","|           |1       |1       |1       |0       |0       |0       |  \n","\n","|unigram(d2)|I       |am      |learning|machine |is      |cool    |\n","|:---------:|:------:|:------:|:------:|:------:|:------:|:------:|\n","|           |0       |0       |1       |1       |1       |1       |\n","  \n","&nbsp;&nbsp;&nbsp;&nbsp;And, the bigrams of d1 and d2 are:\n","  \n","|bigram(d1) |I I     |I am    |I learning|...|machine am|machine learning|...|cool is|cool cool|\n","|:---------:|:------:|:------:|:--------:|:-:|:--------:|:--------------:|:-:|:-----:|:-------:|\n","|           |0       |1       |0         |...|0         |0               |...|0      |0        |  \n","\n","|bigram(d2) |I I     |I am    |I learning|...|machine am|machine learning|...|cool is|cool cool|\n","|:---------:|:------:|:------:|:--------:|:-:|:--------:|:--------------:|:-:|:-----:|:-------:|\n","|           |0       |0       |0         |...|0         |1               |...|0      |0        |"]},{"cell_type":"markdown","metadata":{"id":"KXmN3n163R7w"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;Often, we can achieve slightly better results if instead of counts of words we use something called **term frequency times inverse document frequency** (or **tf-idf**). Maybe it sounds complicated, but it is not. Bear with me, I will explain this. The intuition behind this is the following. So, what's the problem of using just the frequency of terms inside a document? <b>Although some terms may have a high frequency inside documents they may not be so relevant for describing a given document in which they appear. That's because those terms may also have a high frequency across the collection of all documents</b>. For example, a collection of movie reviews may have terms specific to movies/cinematography that are present in almost all documents (they have a high **document frequency**). So, when we encounter those terms in a document this doesn't tell much about whether it is a positive or negative review. We need a way of relating **term frequency** (how frequent a term is inside a document) to **document frequency** (how frequent a term is across the whole collection of documents). That is:  \n","  \n","$$\\begin{align}\\frac{\\text{term frequency}}{\\text{document frequency}} &= \\text{term frequency} \\cdot \\frac{1}{\\text{document frequency}} \\\\ &= \\text{term frequency} \\cdot \\text{inverse document frequency} \\\\ &= \\text{tf} \\cdot \\text{idf}\\end{align}$$  \n","  \n","&nbsp;&nbsp;&nbsp;&nbsp;Now, there are more ways used to describe both term frequency and inverse document frequency. But the most common way is by putting them on a logarithmic scale:  \n","  \n","$$tf(t, d) = log(1+f_{t,d})$$  \n","$$idf(t) = log(\\frac{1+N}{1+n_t})$$  \n","  \n","&nbsp;&nbsp;&nbsp;&nbsp;where:  \n","$$\\begin{align}f_{t,d} &= \\text{count of term } \\textbf{t} \\text{ in document } \\textbf{d} \\\\  \n","N &= \\text{total number of documents} \\\\  \n","n_t &= \\text{number of documents that contain term } \\textbf{t}\\end{align}$$  \n","  \n","<b>We added 1 in the first logarithm to avoid getting $-\\infty$ when $f_{t,d}$ is 0. In the second logarithm we added one fake document to avoid division by zero.</b>"]},{"cell_type":"markdown","metadata":{"id":"rVTHZX0k3R7w"},"source":["Before we transform our data into vectors of counts or tf-idf values we should remove English **stopwords**<sup>(6)(7)</sup>. <b>Stopwords are words that are very common in a language</b> and are usually removed in the preprocessing stage of natural text-related tasks like sentiment analysis or search."]},{"cell_type":"markdown","metadata":{"id":"IQeb6PWR3R7x"},"source":["<b>Note that we should construct our vocabulary only based on the training set. When we will process the test data in order to make predictions we should use only the vocabulary constructed in the training phase, the rest of the words will be ignored.</b>"]},{"cell_type":"markdown","metadata":{"id":"cW2YaeUV3R7x"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;Now, let's create the data frames and save them as csv files:"]},{"cell_type":"markdown","metadata":{"id":"jVrHfM-K3R7x"},"source":["### Text vectorization"]},{"cell_type":"markdown","metadata":{"id":"clrWb80M3R7x"},"source":["Fortunately, for the text vectorization part all the hard work is already done in the Scikit-Learn classes `CountVectorizer`<sup>(8)</sup> and `TfidfTransformer`<sup>(5)</sup>. We will use these classes to transform our csv files into unigram and bigram matrices (using both counts and tf-idf values). (<b>It turns out that if we only use a n-gram for a large n we don't get a good accuracy, we usually use all n-grams up to some n. So, when we say here bigrams we actually refer to uni+bigrams and when we say unigrams it's just unigrams.</b>) Each row in those matrices will represent a document (review) in our dataset, and each column will represent values associated with each word in the vocabulary (in the case of unigrams) or values associated with each combination of maximum 2 words in the vocabulary (bigrams).  \n","\n","&nbsp;&nbsp;&nbsp;&nbsp;`CountVectorizer` has a parameter `ngram_range` which expects a tuple of size 2 that controls what n-grams to include. After we constructed a `CountVectorizer` object we should call `.fit()` method with the actual text as a parameter, in order for it to learn the required statistics of our collection of documents. Then, by calling `.transform()` method with our collection of documents it returns the matrix for the n-gram range specified. As the class name suggests, this matrix will contain just the counts. To obtain the tf-idf values, the class `TfidfTransformer` should be used. It has the `.fit()` and `.transform()` methods that are used in a similar way with those of `CountVectorizer`, but they take as input the counts matrix obtained in the previous step and `.transform()` will return a matrix with tf-idf values. We should use `.fit()` only on training data and then store these objects. When we want to evaluate the test score or whenever we want to make a prediction we should use these objects to transform the data before feeding it into our classifier.  \n","\n","&nbsp;&nbsp;&nbsp;&nbsp;Note that the matrices generated for our train or test data will be huge, and if we store them as normal numpy arrays they will not even fit into RAM. But most of the entries in these matrices will be zero. So, these Scikit-Learn classes are using Scipy sparse matrices<sup>(9)</sup> (`csr_matrix`<sup>(10)</sup> to be more exactly), which store just the non-zero entries and save a LOT of space.  \n","\n","&nbsp;&nbsp;&nbsp;&nbsp;We will use a linear classifier with stochastic gradient descent, `sklearn.linear_model.SGDClassifier`<sup>(11)</sup>, as our model. First we will generate and save our data in 4 forms: unigram and bigram matrix (with both counts and tf-idf values for each). Then we will train and evaluate our model for each these 4 data representations using `SGDClassifier` with the default parameters. After that, we choose the data representation which led to the best score and we will tune the hyper-parameters of our model with this data form using cross-validation in order to obtain the best results.\n","\n","<b>Refs:</b> \n","* Convert a collection of text documents to a matrix of token counts: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n","* Convert a collection of raw documents to a matrix of TF-IDF features: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CPUzkisB3R75","executionInfo":{"status":"ok","timestamp":1660003431691,"user_tz":-180,"elapsed":156,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"zSls98Vh3R75"},"source":["#### Unigram Counts"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"coZ0wYAt3R76","executionInfo":{"status":"error","timestamp":1660003431851,"user_tz":-180,"elapsed":313,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}},"outputId":"cd84ad4c-1df6-4072-e486-e2e1e3b74abc","colab":{"base_uri":"https://localhost:8080/","height":415}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-fa848115f868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"# TRAINING\\n# unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))\\n# unigram_vectorizer.fit(imdb_train['text'].values)\\n# dump(unigram_vectorizer, 'data_preprocessors/unigram_vectorizer.joblib')\\n\\n# TESTING\\nunigram_vectorizer = load('data_preprocessors/unigram_vectorizer.joblib')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'load' is not defined"]}],"source":["%%time\n","# TRAINING\n","# unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))\n","# unigram_vectorizer.fit(imdb_train['text'].values)\n","# dump(unigram_vectorizer, 'data_preprocessors/unigram_vectorizer.joblib')\n","\n","# TESTING\n","unigram_vectorizer = load('data_preprocessors/unigram_vectorizer.joblib')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ySvOJVCk3R76","executionInfo":{"status":"aborted","timestamp":1660003431704,"user_tz":-180,"elapsed":156,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":["unigram_vectorizer.vocabulary_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aplmOQl83R77","executionInfo":{"status":"aborted","timestamp":1660003431740,"user_tz":-180,"elapsed":191,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":["df = pd.DataFrame(unigram_vectorizer.vocabulary_.items(), columns=['Vocabulary', 'Frequency'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3LrZqRoN3R77","executionInfo":{"status":"aborted","timestamp":1660003431749,"user_tz":-180,"elapsed":199,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nwI8hKuK3R77","executionInfo":{"status":"aborted","timestamp":1660003431752,"user_tz":-180,"elapsed":193,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":["df.sort_values(by=\"Frequency\", axis=0, ascending=False, inplace=True, kind='quicksort', na_position='last')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDvQ9FJL3R78","executionInfo":{"status":"aborted","timestamp":1660003431754,"user_tz":-180,"elapsed":194,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":["df.head(n=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQZALboj3R78","executionInfo":{"status":"aborted","timestamp":1660003431759,"user_tz":-180,"elapsed":199,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":["df.tail(n=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXVBZduk3R78","executionInfo":{"status":"aborted","timestamp":1660003431762,"user_tz":-180,"elapsed":201,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":["%%time\n","# TRAINING\n","# X_train_unigram = unigram_vectorizer.transform(imdb_train['text'].values)\n","# save_npz('vectorized_data/X_train_unigram.npz', X_train_unigram)\n","\n","# TESTING\n","X_train_unigram = load_npz('vectorized_data/X_train_unigram.npz')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8zcFKtc3R79","executionInfo":{"status":"aborted","timestamp":1660003431763,"user_tz":-180,"elapsed":167,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"juDnfw8k3R79"},"source":["<b> fit_transform </b>"]},{"cell_type":"markdown","metadata":{"id":"j3eFjD-D3R79"},"source":["#### Unigram Tf-Idf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NSE0GoEN3R79","executionInfo":{"status":"aborted","timestamp":1660003431842,"user_tz":-180,"elapsed":245,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":["%%time\n","# TRAINING\n","# unigram_tf_idf_transformer = TfidfTransformer()\n","# unigram_tf_idf_transformer.fit(X_train_unigram)\n","# dump(unigram_tf_idf_transformer, 'data_preprocessors/unigram_tf_idf_transformer.joblib')\n","\n","# TESTING\n","unigram_tf_idf_transformer = load('data_preprocessors/unigram_tf_idf_transformer.joblib') "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMm0BnTS3R7-","executionInfo":{"status":"aborted","timestamp":1660003431844,"user_tz":-180,"elapsed":242,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":["%%time\n","# TRAINING\n","# X_train_unigram_tf_idf = unigram_tf_idf_transformer.transform(X_train_unigram)\n","# save_npz('vectorized_data/X_train_unigram_tf_idf.npz', X_train_unigram_tf_idf)\n","\n","# TESTING\n","X_train_unigram_tf_idf = load_npz('vectorized_data/X_train_unigram_tf_idf.npz')"]},{"cell_type":"markdown","metadata":{"id":"jaVG_ai43R7-"},"source":["#### Bigram Counts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEJ3seJR3R7-","executionInfo":{"status":"aborted","timestamp":1660003431845,"user_tz":-180,"elapsed":242,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":["%%time\n","# TRAINING\n","# bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n","# bigram_vectorizer.fit(imdb_train['text'].values)\n","# dump(bigram_vectorizer, 'data_preprocessors/bigram_vectorizer.joblib')\n","\n","# TESTING\n","bigram_vectorizer = load('data_preprocessors/bigram_vectorizer.joblib')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9Q8rhBE3R7_","executionInfo":{"status":"aborted","timestamp":1660003431847,"user_tz":-180,"elapsed":243,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":["%%time\n","# TRAINING\n","X_train_bigram = bigram_vectorizer.transform(imdb_train['text'].values)\n","save_npz('vectorized_data/X_train_bigram.npz', X_train_bigram)\n","\n","# TESTING\n","X_train_bigram = load_npz('vectorized_data/X_train_bigram.npz')"]},{"cell_type":"markdown","metadata":{"id":"cLpG5dc33R8A"},"source":["#### Bigram Tf-Idf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzXmFkKw3R8B","executionInfo":{"status":"aborted","timestamp":1660003431848,"user_tz":-180,"elapsed":244,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":["%%time\n","# TRAINING\n","bigram_tf_idf_transformer = TfidfTransformer()\n","bigram_tf_idf_transformer.fit(X_train_bigram)\n","dump(bigram_tf_idf_transformer, 'data_preprocessors/bigram_tf_idf_transformer.joblib')\n","\n","# TESTING\n","bigram_tf_idf_transformer = load('data_preprocessors/bigram_tf_idf_transformer.joblib')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJ2JvFwb3R8B","executionInfo":{"status":"aborted","timestamp":1660003431849,"user_tz":-180,"elapsed":244,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"}}},"outputs":[],"source":["X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)\n","save_npz('vectorized_data/X_train_bigram_tf_idf.npz', X_train_bigram_tf_idf)\n","\n","# X_train_bigram_tf_idf = load_npz('vectorized_data/X_train_bigram_tf_idf.npz')"]},{"cell_type":"markdown","metadata":{"id":"HpS8hMeP3R8C"},"source":["### Choosing data format"]},{"cell_type":"markdown","metadata":{"id":"YsxTxmYO3R8C"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;Now, for each data form we split it into train & validation sets, train a `SGDClassifier` and output the score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2Q6JsY63R8D"},"outputs":[],"source":["def train_and_show_scores(X: csr_matrix, y: np.array, title: str) -> None:\n","    X_train, X_valid, y_train, y_valid = train_test_split(\n","        X, y, train_size=0.75, stratify=y\n","    )\n","\n","    clf = SGDClassifier()\n","    clf.fit(X_train, y_train)\n","    train_score = clf.score(X_train, y_train)\n","    valid_score = clf.score(X_valid, y_valid)\n","    print(f'{title}\\nTrain score: {round(train_score, 2)} ; Validation score: {round(valid_score, 2)}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtEiDzVW3R8D"},"outputs":[],"source":["y_train = imdb_train['label'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"khzjqGou3R8D"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aks1wBAa3R8E","outputId":"8daf3dba-4199-46c2-cc57-27d422cb63e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unigram Counts\n","Train score: 1.0 ; Validation score: 0.87\n","\n","Unigram Tf-Idf\n","Train score: 0.95 ; Validation score: 0.89\n","\n","Bigram Counts\n","Train score: 1.0 ; Validation score: 0.89\n","\n","Bigram Tf-Idf\n","Train score: 0.98 ; Validation score: 0.9\n","\n"]}],"source":["train_and_show_scores(X_train_unigram, y_train, 'Unigram Counts')\n","train_and_show_scores(X_train_unigram_tf_idf, y_train, 'Unigram Tf-Idf')\n","train_and_show_scores(X_train_bigram, y_train, 'Bigram Counts')\n","train_and_show_scores(X_train_bigram_tf_idf, y_train, 'Bigram Tf-Idf')"]},{"cell_type":"markdown","metadata":{"id":"-Q7fW0vN3R8E"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;The best data form seems to be **bigram with tf-idf** as it gets the highest validation accuracy: **0.9**; we will use it next for hyper-parameter tuning."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xYO9_SUX3R8E"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bI5iBGuq3R8F"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"wIDNJ7Zw3R8F"},"source":["<h1> TUTORIAL </h1>"]},{"cell_type":"markdown","metadata":{"id":"GNtAhtB53R8F"},"source":["<h2>Using the processed twitter data from yesterday's challenge</h2>.\n","\n","\n","- Form a new data frame (named `cleanTweet`), containing columns $\\textbf{clean-text}$ and $\\textbf{polarity}$.\n","\n","- Write a function `text_category` that takes a value `p` and returns, depending on the value of p, a string `'positive'`, `'negative'` or `'neutral'`.\n","\n","- Apply this function (`text_category`) on the $\\textbf{polarity}$ column of `cleanTweet` in 1 above to form a new column called $\\textbf{score}$ in `cleanTweet`.\n","\n","- Visualize The $\\textbf{score}$ column using piechart and barchart\n","\n","<h5>Now we want to build a classification model on the clean tweet following the steps below:</h5>\n","\n","* Remove rows from `cleanTweet` where $\\textbf{polarity}$ $= 0$ (i.e where $\\textbf{score}$ = Neutral) and reset the frame index.\n","* Construct a column $\\textbf{scoremap}$ Use the mapping {'positive':1, 'negative':0} on the $\\textbf{score}$ column\n","* Create feature and target variables `(X,y)` from $\\textbf{clean-text}$ and $\\textbf{scoremap}$ columns respectively.\n","* Use `train_test_split` function to construct `(X_train, y_train)` and `(X_test, y_test)` from `(X,y)`\n","\n","* Build an `SGDClassifier` model from the vectorize train text data. Use `CountVectorizer()` with a $\\textit{trigram}$ parameter.\n","\n","* Evaluate your model on the test data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGq1lByX3R8F"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j7ZATPzL3R8F"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3C_Cjhhz3R8G"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"jA3JeOJy3R8G"},"source":["# EXTENSION"]},{"cell_type":"markdown","metadata":{"id":"5ZOh14DL3R8G"},"source":["### Using Cross-Validation for hyperparameter tuning"]},{"cell_type":"markdown","metadata":{"id":"_2kJNmnG3R8G"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;For this part we will use `RandomizedSearchCV`<sup>(12)</sup> which chooses the parameters randomly from the list that we give, or according to the distribution that we specify from `scipy.stats` (e.g. uniform); then is estimates the test error by doing cross-validation and after all iterations we can find the best estimator, the best parameters and the best score in the variables `best_estimator_`, `best_params_` and `best_score_`.  \n","\n","&nbsp;&nbsp;&nbsp;&nbsp;Because the search space for the parameters that we want to test is very big and it may need a huge number of iterations until it finds the best combination, we will split the set of parameters in 2 and do the hyper-parameter tuning process in two phases. First we will find the optimal combination of loss, learning_rate and eta0 (i.e. initial learning rate); and then for penalty and alpha."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oh_r5PQA3R8H"},"outputs":[],"source":["X_train = X_train_bigram_tf_idf"]},{"cell_type":"markdown","metadata":{"id":"hZuCc-_H3R8H"},"source":["#### Phase 1: loss, learning rate and initial learning rate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IcVKjN3M3R8H"},"outputs":[],"source":["clf = SGDClassifier()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SntR42AR3R8H"},"outputs":[],"source":["distributions = dict(\n","    loss=['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n","    learning_rate=['optimal', 'invscaling', 'adaptive'],\n","    eta0=uniform(loc=1e-7, scale=1e-2)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4bEFeZe13R8I"},"outputs":[],"source":["random_search_cv = RandomizedSearchCV(\n","    estimator=clf,\n","    param_distributions=distributions,\n","    cv=5,\n","    n_iter=50\n",")\n","random_search_cv.fit(X_train, y_train)\n","print(f'Best params: {random_search_cv.best_params_}')\n","print(f'Best score: {random_search_cv.best_score_}')"]},{"cell_type":"markdown","metadata":{"id":"sPT0Qlpv3R8I"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;Because we got \"learning_rate = optimal\" to be the best, then we will ignore the eta0 (initial learning rate) as it isn't used when learning_rate='optimal'; we got this value of eta0 just because of the randomness involved in the process."]},{"cell_type":"markdown","metadata":{"id":"-JkgOEAK3R8I"},"source":["#### Phase 2: penalty and alpha"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XkI1yE0E3R8I"},"outputs":[],"source":["clf = SGDClassifier()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oy0DXpdE3R8J"},"outputs":[],"source":["distributions = dict(\n","    penalty=['l1', 'l2', 'elasticnet'],\n","    alpha=uniform(loc=1e-6, scale=1e-4)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnRXgu_X3R8J"},"outputs":[],"source":["random_search_cv = RandomizedSearchCV(\n","    estimator=clf,\n","    param_distributions=distributions,\n","    cv=5,\n","    n_iter=50\n",")\n","random_search_cv.fit(X_train, y_train)\n","print(f'Best params: {random_search_cv.best_params_}')\n","print(f'Best score: {random_search_cv.best_score_}')"]},{"cell_type":"markdown","metadata":{"id":"R721JzOc3R8J"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;So, the best parameters that I got are:  \n","`loss: squared_hinge  \n"," learning_rate: optimal  \n"," penalty: l2  \n"," alpha: 1.2101013664295101e-05  `"]},{"cell_type":"markdown","metadata":{"id":"tp1Mx4sB3R8J"},"source":["#### Saving the best classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kdTyjK5k3R8K"},"outputs":[],"source":["sgd_classifier = random_search_cv.best_estimator_\n","\n","dump(random_search_cv.best_estimator_, 'classifiers/sgd_classifier.joblib')\n","\n","# sgd_classifier = load('classifiers/sgd_classifier.joblib')"]},{"cell_type":"markdown","metadata":{"id":"9mTuo5rS3R8K"},"source":["### Testing model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MaUJHuud3R8K"},"outputs":[],"source":["X_test = bigram_vectorizer.transform(imdb_test['text'].values)\n","X_test = bigram_tf_idf_transformer.transform(X_test)\n","y_test = imdb_test['label'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQXL7no33R8K"},"outputs":[],"source":["score = sgd_classifier.score(X_test, y_test)\n","print(score)"]},{"cell_type":"markdown","metadata":{"id":"HRylCcqp3R8L"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;And we got **90.18%** test accuracy. That's not bad for our simple linear model. There are more advanced methods that give better results. The current state-of-the-art on this dataset is **97.42%** <sup>(13)</sup>"]},{"cell_type":"markdown","metadata":{"id":"PP-GTVdU3R8L"},"source":["# Deployment\n","* Flask\n","* Streamlit"]},{"cell_type":"markdown","metadata":{"id":"iEfBc9zu3R8L"},"source":["# References\n","\n","<sup>(1)</sup> &nbsp;[Sentiment Analysis - Wikipedia](https://en.wikipedia.org/wiki/Sentiment_analysis)  \n","<sup>(2)</sup> &nbsp;[Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)  \n","<sup>(3)</sup> &nbsp;[Bag-of-words model - Wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)  \n","<sup>(4)</sup> &nbsp;[Tf-idf - Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)  \n","<sup>(5)</sup> &nbsp;[TfidfTransformer - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)  \n","<sup>(6)</sup> &nbsp;[Stop words - Wikipedia](https://en.wikipedia.org/wiki/Stop_words)  \n","<sup>(7)</sup> &nbsp;[A list of English stopwords](https://gist.github.com/sebleier/554280)  \n","<sup>(8)</sup> &nbsp;[CountVectorizer - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)  \n","<sup>(9)</sup> &nbsp;[Scipy sparse matrices](https://docs.scipy.org/doc/scipy/reference/sparse.html)  \n","<sup>(10)</sup> [Compressed Sparse Row matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix)  \n","<sup>(11)</sup> [SGDClassifier - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)  \n","<sup>(12)</sup> [RandomizedSearchCV - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)  \n","<sup>(13)</sup> [Sentiment Classification using Document Embeddings trained with\n","Cosine Similarity](https://www.aclweb.org/anthology/P19-2057.pdf)  "]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"sentiment-analysis.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}